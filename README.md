# self_MT

马尔科夫性，是俄国数学家安德雷·马尔科夫得名。是指未来状态的概率分布仅与当前状态有关，与过去状态条件独立。
机器翻译解码过程不具备马尔科夫性，因此动态规划算法不适用于解码过程。启发式搜索beam search 是目前实验效果较理想的解码策略，但贪心算法
并不一定能得到全局最优序列。
通过分析，概率差异标准化，在解码过程，采用动态beam size，对无效节点进行剪枝，又保留了获得全局最优解的可能性。
这里采用 Bert 语言模型解码，只是引入了语言模型的预测，
      像：我喜欢吃  后面是苹果的概率大还是香蕉的概率大，由训练语言模型的语料决定，与源语言的上下文信息无关。采用跨语言语言模型是否会更好。

克林闭包

解码错误，或问题较多的地方是被BPE切分的地方。
分析罕见词，分词结果
            BPE 切分结果
            翻译结果 之间的关联
猜想：中英翻译
      直接分词训练，unk，词表大小问题。
      sentencepiece-unigram 效果是否与直接分词训练结果差不大。
  中文句子切分为更细粒度，比如char 是否会使得中英翻译效果更好。因为对sentencepiece-unigram，再次进行bpe分词，其他条件不变，效果变好了。还有分词后+bpe切分也只不过把词切的更细。
  或者中文切分粒度与英文单词的意思对应上，英文译文指导中文分词。
  测试 sentecepiece-unigram-bpe，效果与 sentecepiece-unigram 比有提升，查看词表；猜测中英翻译，unigram 切分单词较长，导致中到英一对多翻译；分词后的bpe切分更细，中英翻译多对一翻译，较容易学习映射关系；
  nist 测试集 BLEU 得分区别不大，研报测试集有提升，猜测研报测试集与通用测试集相比，中文分词导致的问题更多？
  不采用分词，或者说是高频词切分+低频词采用char类型训练。
  
  翻译解码时，高频词对应高频词，中频词对应中频词，低频词对应低频词，用来剪枝，或者不同领域词加权
  翻译解码时，ffn 的隐藏层维度设置得与目标端词表一致，翻译效果是否会更好。
  
  正向翻译语料的质量，使得模型效果快速逼近google翻译。
  
  基于图输入的神经机器翻译模型，再增加一组向量或矩阵，来表示单词之间的依赖关系；作为机器翻译的 embedding 输入。
  
  齐夫定律(Zipf's law), 一个词在一个有相当长度的语篇中的等级序号（该词在按出现次数排列的词表中的位置，他称之为rank，简称r）与该词的出现频率（他称为frequency，简称f）的乘积几乎是一个常数（constant，简称C）。用公式表示，就是 r × f = C 。
  基于词频指导的对比学习，解码时，源语输入的句子加上词频这一维度。
  
机器翻译应遵循的原则：
1. 对等原则，中英文训练语料对齐。

模型训练：
1. base模型，平行语料+正向语料+反向语料（各个领域），混合训练。
2. 反向训练？领域训练？高质量语料训练？
3.

机器翻译训练技巧：
1. 中英模型，中英BPE模型单独训练，迭代次数少一点好，25K。
2. dropout 调小会缓解 fairseq NON 梯度消失问题，可以继续训练。
3. 最后一批微调数据要高质量。
4. 数字，标点之间全部切开，分词后用特殊符号标记，方便恢复。
5. 训练平行语料全部过一遍术语抽取，再来训练（待验证）。
